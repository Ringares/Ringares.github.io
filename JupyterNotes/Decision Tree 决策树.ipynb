{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说, 一颗决策树包含一个根结点, 若干中间结点, 和若干叶结点. \n",
    "\n",
    "- 叶结点 对应决策结果\n",
    "- 其它结点 对应一个属性测试\n",
    "\n",
    "每个结点包含的样本集合根据属性测试的结果被划分到子结点中; 根结点包含所有样本. \n",
    "\n",
    "决策树学习的目的是生成一个泛化能力强的决策树, 其基本流程遵循\"分而治之\"策略."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成算法\n",
    "\n",
    "- 输入: 训练集 $D=\\{(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)\\}$ 和 属性集 $A=\\{a_1, a_2, ..., a_d\\}$\n",
    "- TreeGenerate(D, A)\n",
    "- 输出: 以 node 为根结点的决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    1. 生成结点 node\n",
    "    2. if 训练集D中样本全都属于同一个类别C then\n",
    "           将node标记为属于C类别的叶结点 return (1)\n",
    "       endif\n",
    "    3. if 属性集A为空集 OR D所有样本在所有属性A上的取值都相同 then\n",
    "           将node标记叶结点, 其类别为D中样本数最多的那个分类C return (2)\n",
    "       endif\n",
    "    4. 从属性集A中选择最优划分属性 a*\n",
    "    5. for 属性a*中的每一个值 a*_v do\n",
    "           在 node 下生成一个分支结点, 令D_v表示D的子集, 子集中的样本的属性 a* 的取值为 a*_v\n",
    "           if D_v是空集 then\n",
    "               将分支结点标记为叶结点, 类别C= D中样本最多的分类 return (3)\n",
    "           else\n",
    "               TreeGenerate(D_v, A\\{a*}) // A中除去 a* 以外的集合 \n",
    "           end if\n",
    "\n",
    "       endfor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所述, 在决策树的基本算法中, 有三种情况会导致递归返回.\n",
    "- (1) 当前结点样本都属于同一类别, 无需划分\n",
    "- (2) 当前属性集为空或所有样本在所有属性上取值相同, 无法划分\n",
    "- (3) 当前样本集为空, 无法划分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 决策树算法的关键在于如何选择最优的划分属性\n",
    "\n",
    "> 一般而言, 随着划分的过程不断进行, 我们希望分支结点所包含的样本尽可能的属于同一类别\n",
    "\n",
    "> 也就是结点的纯度越来越高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Entropy 信息熵\n",
    "\n",
    "信息熵用来度量集合样本纯度, 假设当前集合 $D$ 中, 第 $k$ 类样本的比例为 $p_k(k=1,2,...,|y|)$, 则 $D$ 的信息熵定义为\n",
    "\n",
    "$$Ent(D) = - \\sum_{k=1}^{|y|}p_klog_2p_k$$\n",
    "\n",
    "$Ent(D)$ 的值越小, 集合 $D$ 纯度越高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain 信息增益 [ID3]\n",
    "\n",
    "假设离散属性 $a$ 有 $V$ 个可能的取值 $\\{a^1, a^2,..., a^V\\}$, 如果用属性 $a$ 来对样本进行划分, 就会产生 $V$ 个分支结点, 其中第 $v$ 个分支结点包含了 $D$ 中所有在属性 $a$ 上取值为 $a^v$ 的样本, 这个子集记为 $D^v$\n",
    "\n",
    "那么我们可以算出每个分支结点下的信息熵, 并且**因为每个分支结点所含的样本数量不同**, 需要赋予权重 \n",
    "\n",
    "$$|D^v|/|D|$$\n",
    "\n",
    "由此可以计算出, 用属性 $a$ 进行划分所获得的信息增益.\n",
    "\n",
    "$$Gain(D,a) = Ent(D) - \\sum_{v=1}^{V}\\frac{|D^v|}{|D|}Ent(D^v)$$\n",
    "\n",
    "信息增益越大, 说明用这个属性进行划分所带来的纯度提升越大\n",
    "\n",
    "> ID3 决策树算法就是基于信息增益, 用信息增益最大的属性来进行划分.\n",
    "\n",
    "> $a_* = \\arg\\max\\limits_{a\\in{A}}  Gain(D, a)$\n",
    "\n",
    "> 一般来说信息增益偏好于选择取值数目多的属性, 因为可取值越多, 分支下的样本就越纯, 进而熵越小, 信息增益越大.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain Ratio 增益率 [C4.5]\n",
    "\n",
    "为了减少信息增益对多取值属性的偏好所带来的不利影响, 著名的 **C4.5 决策树算法** 使用增益率来选取最优划分属性.\n",
    "\n",
    "$$Gain\\_ratio = \\frac{Gain(D,a)}{IV(a)}$$\n",
    "$$IV(a) = - \\sum_{v=1}^V \\frac{|D^v|}{|D|}log_2\\frac{|D^v|}{|D|}$$\n",
    "\n",
    "$IV(a)$ 被称为属性 $a$ 的固有值 (intrinsic value), 可取值数目越多 (V越大), $IV(a)$ 的值通常会越大 \n",
    "\n",
    "> 增益率策略通常对可取值数目较少的属性有所偏好; 所以 **C4.5** 是先选取信息增益大于平均水平的属性, 再在其中选择增益率最高的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Index 基尼指数 [CART]\n",
    "\n",
    "CART 决策树 (Classification And Regression Tree) 使用基尼指数来选择划分属性, 数据集 $D$ 的基尼值表示为\n",
    "\n",
    "$$Gini(D) = \\sum_{k=1}^{|y|}\\sum_{k'\\not = k}^{|y|}p_k^2$$\n",
    "$$Gini(D) = 1 - \\sum_{k=1}^{|y|}p_k^2$$\n",
    "\n",
    "直观来说 $Gini(D)$ 反应了从数据集中随机抽取两个样本, 其类别不一致的概率. 所以 $Gini(D)$ 越小, 数据集纯度越高. 我们可以把用属性 $a$ 的基尼指数定义为\n",
    "\n",
    "$$Gini\\_index(D,a) = \\sum_{v=1}^V \\frac{|D^v|}{|D|}Gini(D^v)$$\n",
    "\n",
    "> 因此, 我们选择划分后基尼指数最大的属性作为最优划分属性\n",
    "\n",
    "> $a_* = \\arg\\min\\limits_{a \\in A} Gini\\_index(D,a) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 剪枝处理\n",
    "\n",
    "剪枝是决策树算法中对抗\"过拟合\"的主要手段, 通过主动去掉一些分支来降低过拟合的风险. 通过判断剪枝前后, 树的泛化性有没有提升, 来决定是否在这个结点进行剪枝, 直接标记为叶结点. 而我们可以采用预留法, 将数据集划分为 训练集 (training set)和 验证集(cv set), 通过模型对验证集的拟合程度, 来判断树的泛化性能."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-pruning 预剪枝\n",
    "\n",
    "预剪枝是在树的生成过程中, 每次用属性划分分支结点后, 用验证集来判断划分前后的泛化性能, 如果验证集的精度没有提升, 就不进行划分, 直接标记为叶结点. (如果划分前后, 验证集的精度没有变化, 根据奥卡姆剃刀, 简单的模型更好; 如果考虑保守策略, 也可以不剪枝, 继续划分下去)\n",
    "\n",
    "因为在树生成过程中, 很多分支有可能就没有展开, 所以预剪枝的时间开销比较小, 但是通常模型比较简单, 所以有过拟合的风险."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-pruning 后剪枝\n",
    "\n",
    "后剪枝是在树生成后, 逆向的一个个检查分支结点, 同样的, 判断分支结点转为叶结点前后, 验证集的精度是否有提高.\n",
    "\n",
    "通常后剪枝会保留更多的分支, 后剪枝决策树的过拟合风险较小, 但是训练的时间开销比预剪枝和未剪枝的决策树都要大."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 连续值处理\n",
    "\n",
    "当我们处理连续属性的时候, 需要把连续属性转为离散属性, 最简单的策略是使用二分法 [C4.5].\n",
    "\n",
    "假设有连续属性 $a$, 样本集 $D$ 在属性 $a$ 上有 $n$ 个取值 $\\{a^1, a^2, ..., a^n\\}$, 我们需要 $n-1$ 个划分点, 每个划分点 t 都会把样本集 $D$ 划分为两部分, $D^-$ 为取值不大于 $t$ 的子集; $D^+$ 为取值大于 $t$ 的子集. 因此, 我们把划分点看做独立的离散属性来处理.\n",
    "\n",
    "具体的来说, 首先对所有样本在连续属性 $a$ 上的值进行由小到大的排序, 然后每个划分点, 通过二分法, 就是每两个相邻属性值的中间点.\n",
    "\n",
    "$$T_a = \\{ \\frac{a_1+a_2}{2}, \\frac{a_2+a_3}{2}, ..., \\frac{a_{n-1}+a_n}{2} \\}$$\n",
    "\n",
    "接下来,就是把 $T_a$ 看做离散属性集, 来选择最优的划分属性. 值得注意的是, 与离散属性同, 连续属性 $a$ 在当前结点用过后, 在接下去的子结点还可以使用."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 缺失值处理\n",
    "\n",
    "决策树算法有能力处理有缺失值的数据, 我们来看一下面对有数据缺失时,需要面对的两个问题.\n",
    "\n",
    "1. 如何在属性值缺失的情况下, 选择最优的划分属性\n",
    "2. 给定了划分属性, 当样本在这个属性上的值有缺失时, 应该怎样划分样本\n",
    "\n",
    "假设 属性 $a$ 有 $V$ 个可取值 $\\{a^1, a^2, ..., a^V\\}$\n",
    "\n",
    "- $\\tilde{D}$ : 样本集 $D$ 中, 在属性 $a$ 上的值没有缺失的样本子集\n",
    "- $\\tilde{D}_v$ : $\\tilde{D}$ 中, 属性 $a=a^v$ 的样本子集 ( $v=1, 2, ..., V$ )\n",
    "- $\\tilde{D}_k$ : $\\tilde{D}$ 中, 分类为 $k$ 的样本子集 ( $k=1, 2, ..., |y|$ )\n",
    "- 假设个每个样本 $x$ 都赋予一个权重 $w_x$, 初始值为 1\n",
    "\n",
    "我们定义一下三个值, 直观的理解: 对于属性 $a$\n",
    "\n",
    "- $\\rho$ 表示无缺失值样本的比例; \n",
    "- $\\tilde{p}_k$ 表示无缺失值样本中, 第 $k$ 类样本占的比例; \n",
    "- $\\tilde{r}_v$ 表示无缺失值样本中, $a$ 上取值为 $a^v$ 的样本的比例\n",
    "\n",
    "$$\\rho = \\frac{\\sum_{x\\in{\\tilde{D}}}w_x}{\\sum_{x\\in{D}}w_x}$$\n",
    "\n",
    "$$\\tilde{p}_k = \\frac{\\sum_{x\\in{\\tilde{D}_k}}w_x}{\\sum_{x\\in{\\tilde{D}}}w_x}$$\n",
    "\n",
    "$$\\tilde{r}_v = \\frac{\\sum_{x\\in{\\tilde{D}_v}}w_x}{\\sum_{x\\in{\\tilde{D}}}w_x}$$\n",
    "\n",
    "对于问题 1 **如何在属性值缺失的情况下, 选择最优的划分属性**, 我们只能根据 $\\tilde{D}$ 来选择最优划分属性, 将信息增益的公式推广为\n",
    "\n",
    "$$Gain(D,a) = \\rho \\times Grain(\\tilde{D},a)$$\n",
    "\n",
    "$$Gain(D,a) = \\rho \\times \\lgroup Ent(\\tilde{D},a) - \\sum_{v=1}^{V}\\tilde{r}_v Ent(\\tilde{}_v) \\rgroup$$\n",
    "\n",
    "$$Ent(D̃) = - \\sum_{k=1}^{|y|} \\tilde{p}_k log_2 \\tilde{p}_k$$\n",
    "\n",
    "对于问题 2 **给定了划分属性, 当样本在这个属性上的值有缺失时, 应该怎样划分样本**, 分为两种情况, 如果样本没有属性值缺失, 那么就划分到对应属性值的子结点里, 保持样本权重 $w_x$ 不变; 如果样本在属性上的值有缺失, 那么把这个样本划分到所有子结点里, 并且更新样本权重值为 $\\tilde{r}_v \\cdot w_x$ (直观理解, 让样本以不同的概率划分到不同的子结点里)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 多属性决策树\n",
    "\n",
    "如果把样本的一个属性看做坐标空间中的一个坐标轴, 那么一个有 d 个属性的样本, 就可以被描述成在 d 维空间中的一个点. 而普通决策树, 每一个分支结点的划分, 都是根据单一属性划分, 也就是每一个分类条件边界都是 **轴平行** 的. 进而整颗树的分类边界也是轴平行的, 由若干个与坐标轴平行的分段组成. 这种情况, 在真实分类边界比较复杂时, 必须使用很多段才能获得一个比较好的近似, 从而导致大量的属性测试, 决策树会变得非常复杂, 预测的时间开销也会很大."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/decision_tree_bound.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多属性决策树, 再分支的划分上, 不再只考虑单个属性, 而是多个属性的组合. 例如下图的斜线的分类边界, 就是两个属性的线性组合; 这样的话, 决策树的模型会得到简化. 多属性决策树, 不是为每个分支结点找到最有的划分属性, 而是寻找一个最合适的线性分类器."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/multivariate_decision_tree_bound.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 随机森林 集成学习\n",
    "\n",
    "Bagging [Breiman, 1996a] 是并行式集成学习方法最著名的代表. 而随机森林 Random Forest(RF) [Breiman, 2001a] 是 Bagging 的一个扩展变体. RF 是在意决策树为基学习器构建 Bagging 集成的基础上, 进一步在决策树的训练过程中, 引入了随机属性选择. 具体来说, 就是在分支结点选择最优划分属性的时候, 假设当前属性集合有 d 个属性, 先从当前属性集合中随机选择 k 个属性, 再从这 k 个属性的子集中选出最优的划分属性. \n",
    "\n",
    "- 若令 k = d, 则基决策树的构建方式与传统决策树相同\n",
    "- 若令 k = 1, 则是随机选择一个属性进行划分\n",
    "- 一般来说, 推荐 $k=log_2d$ [Breiman, 2001a]\n",
    "\n",
    "随机森林中基学习器的多样性与 Bagging 中的相比, 不仅来自于**样本扰动**, 而且还来自于**属性扰动**. 这就使得最终集成的泛化性能,可能通过个体基学习器之间差异度的增加, 而进一步提高.\n",
    "\n",
    "随机森林的收敛性和 Bagging 相似. 通常随机森林的起始性能相对较差, 尤其是基学习器数量较少的时候, 问题属性扰动的加入, 往往使单个学习器的性能降低; 然而随着个体学习器数目的增加, **随机森林通常会收敛到更低的泛化误差**. \n",
    "\n",
    "而且**随机森林的训练效率也是优于 Bagging** 的, 因为 Bagging 在构建基学习器的时候使用的是确定的决策树, 要考察所有的属性, 而随机森林中只需要考察一个属性子集."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding by Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart\n",
    "    \n",
    "sklearn.tree.DecisionTreeClassifier http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "        \n",
    "Understanding the decision tree structure http://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[2,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "clf2 = tree.DecisionTreeClassifier()\n",
    "clf2 = clf2.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the probability of each class can be predicted, which is the fraction of training samples of the same class in a leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.predict_proba(iris.data[1:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `sklearn.model_selection.cross_val_score` 衡量模型质量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95925925925925926"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(clf2, iris.data, iris.target, cv=15).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf3 = RandomForestClassifier(n_estimators=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 `clf.feature_importances_` 来查看属性的重要程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x116985dd8>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc8AAAD8CAYAAAAR4S+cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11697dba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "features = pd.DataFrame()\n",
    "features['feature'] = iris.feature_names\n",
    "features['importance'] = clf3.feature_importances_\n",
    "features.sort_values(by=['importance'], ascending=True, inplace=True)\n",
    "features.set_index('feature', inplace=True)\n",
    "features.plot(kind=\"barh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95925925925925926"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf3, iris.data, iris.target, cv=15).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
