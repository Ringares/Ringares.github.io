{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持向量机 (Support Vector Machine) 是 Cortes 和 Vapnik 于1995年首先提出的, 它在解决小样本, 非线性及高维模式识别中表现出许多特有的优势, 并能够推广应用到函数拟合等其他机器学习问题中."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 函数间隔\n",
    "\n",
    "给定一个样本 $(x^{(i)}, y^{(i)})$, 函数间隔定义如下\n",
    "\n",
    "$$\n",
    "y^{(i)} = \\begin{cases}\n",
    "   1 &\\text{, if } w^Tx+b \\geq 0  \\\\\n",
    "   -1 &\\text{, if } w^Tx+b < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\\begin{aligned}\n",
    "\\text{函数间隔 } \\hat{\\gamma}^{(i)} &= y^{(i)} (w^Tx+b) \\\\\n",
    "    &= | w^Tx+b |\n",
    "\\end{aligned}\n",
    "\n",
    "为了使函数间隔最大, 也就是为了更有信心确认其分类. \n",
    "\n",
    "- 如果是正例 $(y=1)$, 我们希望 $w^Tx+b$ 是大正数;\n",
    "- 如果是反例 $(y=-1)$, 我们希望 $w^Tx+b$ 是大负数.\n",
    "\n",
    "全局上的函数间隔为所有样本的函数间隔中最小的, 分类时确信度最小的函数间隔.\n",
    "\n",
    "$$ \\hat{\\gamma} = \\min_{i=1,2...m} \\hat{\\gamma}^{(i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 几何间隔\n",
    "\n",
    "在样本空间中, 用来进行划分的超平面, 可以用线性方程描述为 $w^Tx+b = 0$, 其中法向量 $w$ 决定了平面的方向, 位移项 $b$ 决定了超平面到原点的距离.\n",
    "\n",
    "几何间隔就是样本到这个超平面的距离\n",
    "\n",
    "$$\\text{几何间隔 }\\gamma^{(i)} = \\frac{|w^Tx^{(i)}+b|}{\\|w\\|}$$\n",
    "\n",
    "同样的, 全局几何间隔为\n",
    "\n",
    "$$ \\gamma = \\min_{i=1,2...m} \\gamma^{(i)} $$\n",
    "\n",
    "可见函数间隔和几何间隔有如下关系, 也就是函数间隔归一化后就等于几何间隔\n",
    "\n",
    "$$ \\gamma = \\frac{\\hat{\\gamma}}{\\|w\\|} $$\n",
    "\n",
    "因此 SVM 只考虑最近的间隔, 所以只考虑最靠近分割平面的点, 这些点就是支持向量.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求最大间隔 (转化为凸优化问题)\n",
    "\n",
    "支持向量机的基本思想就是, 求得能正确划分数据集, 并且有最大几何间隔的超平面. 可以将此问题描述为:\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\max_{w,b} \\text{ } &: \\frac{\\hat{\\gamma}}{\\|w\\|} \\\\\n",
    "    s.t. &: y^{(i)} (w^Tx^{(i)}+b) \\geq \\hat{\\gamma}, (i=1,2,...,m)\n",
    "\\end{aligned}\n",
    "\n",
    "这个时候,目标函数还不是凸函数, 还需要改写. 前面说到同时扩大函数间隔的 $w$ 和 $b$, 对结果没有影响, 但我们最后求的是 $w$ 和 $b$ 的确定值, 所有需要对 $\\hat{\\gamma}$ 做一些限制. 为了方便起见, 令 $\\hat{\\gamma} = 1$, 也就是我们的几何间隔, (支持向量的点到平面的距离)设为 $\\frac{1}{\\|w\\|}$. 进一步, 求 $\\frac{1}{\\|w\\|}$ 的最大值, 等同于求 $\\frac{1}{2}\\|w\\|^2$ 的最小值. 所以上面的式子转化为如下 (凸二次规划问题):\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\min_{w,b} \\text{ } &: \\frac{1}{2}\\|w\\|^2 \\\\\n",
    "    s.t. &: y^{(i)} (w^Tx^{(i)}+b) \\geq 1, (i=1,2,...,m)\n",
    "\\end{aligned}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拉格朗日乘子法 和 KKT条件\n",
    "\n",
    "如果熟悉拉格朗日乘子法, 对偶问题 和 KKT 条件的, 可以直接跳过到 **[SVM 问题的推导]**.\n",
    "\n",
    "通过上一节我们得出的求解最优的分隔超平面的式子, 可以看到是一个有不等式约束的优化问题. 我先来看看一般优化问题的求解思路:\n",
    "\n",
    "Reference\n",
    "- [约束优化方法之拉格朗日乘子法与KKT条件](https://www.cnblogs.com/ooon/p/5721119.html)\n",
    "- [KKT.pdf](http://www.csc.kth.se/utbildning/kth/kurser/DD3364/Lectures/KKT.pdf)\n",
    "\n",
    "### 无条件约束问题\n",
    "\n",
    "对于变量 $x\\in\\Re^N$ 的函数 $f(x)$, 无约束优化问题如下:\n",
    "\n",
    "$$\\min\\limits_x \\text{ }: f(x)$$\n",
    "\n",
    "只有目标函数, 没有约束条件, 那么只需要求得目标函数的导数 $\\nabla_x f(x) = 0$, 如果没有解析解的话, 可以使用梯度下降或牛顿方法等迭代的手段逐步逼近最小值."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 等式约束\n",
    "\n",
    "对目标函数加上等式的约束条件后, 优化问题表示如下:\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\min\\limits_x \\text{ } &: f(x) \\\\\n",
    "    s.t. &: h_i(x) = 0, (i=1,2,...,m)\n",
    "\\end{aligned}\n",
    "\n",
    "约束条件把可取值的范围限定在可行域中, 这种情况下, 不一定能找到 $\\nabla_x f(x) = 0$ 的点, 只能找到在可行域中, 使 $f(x)$ 最小的点. 常用方法为拉格朗日乘子法 $\\alpha\\in\\Re^m$, 引入拉格朗日乘子, 构建拉格朗日函数:\n",
    "\n",
    "$$\\text{拉格朗日函数 } L(x,\\alpha) = f(x) + \\sum_{i=1}^m \\alpha_ih_i(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Equality Constraints Optimization.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拉格朗日乘子法取得极值的条件是目标函数与约束函数相切，这时梯度是相同方向, 或相反的. \n",
    "\n",
    "$$\\text{所以有 } \\nabla_xf(x) + \\alpha\\nabla_xh(x) = 0 $$\n",
    "\n",
    "$$\\text{也就是求 } \\nabla_xL(x,\\alpha) = 0 $$\n",
    "\n",
    "也就是把 $f(x)$ 的等式约束问题转化成了 $L(x,\\alpha)$ 的 无约束问题\n",
    "\n",
    "$$\\min\\limits_x \\text{ } : L(x,\\alpha)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不等式约束\n",
    "\n",
    "不等式约束 (单个约束条件) 的优化问题表示如下:\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\min\\limits_x \\text{ } &: f(x) \\\\\n",
    "    s.t. &: g(x) \\leq 0\n",
    "\\end{aligned}\n",
    "\n",
    "$$L(x,\\lambda) = f(x) + \\lambda g(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Unequality Constraints Optimization.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上图可见, 这是的可行解必须落在约束范围 $g(x)$ 以内, 这时可以考虑两种情况:\n",
    "\n",
    "- 当可行解落在 $g(x)<0$ 的区域内, 这是约束范围相当于不起作用, 和无约束一样, 直接求 $f(x)$ 的最小值就可以了 [令$\\lambda=0$, 消去约束]\n",
    "- 当可行解落在 $g(x)=0$ 也就是边界上市, 就相当于等式约束的问题了\n",
    "\n",
    "这两种情况对于拉格朗日函数 $L$ 来说, 可以都统一为 $\\text{ }\\lambda g(x) = 0$\n",
    "\n",
    "另一个问题是 $\\lambda$ 的取值. 和等式约束不同, 等式约束只需要保证目标函数和约束函数的梯度平行即刻(同向或反向); 而不等式约束问题中, 如果 $\\lambda\\neq0$, 说明是在约束范围边界上的可行解, 那么需要选择远离约束区域, 靠近无约束时的解. 此时约束函数和目标函数的梯度方向应该正好相反.\n",
    "\n",
    "$$-\\nabla_x f(x) = \\lambda\\nabla_x g(x) , \\text{ } \\lambda > 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Unequality Constraints Optimization2.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KKT 条件\n",
    "\n",
    "对于不等式约束，只要满足一定的条件，依然可以使用拉格朗日乘子法解决，这里的条件便是 KKT 条件。\n",
    "\n",
    "先把上面的优化问题推广一下, 给出形式化的不等式约束优化问题\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\min\\limits_x \\text{ } &: f(x) \\\\\n",
    "    s.t. &: h_i(x) = 0 , (i = 1,2,...,m)\\\\\n",
    "         &: g_j(x) \\leq 0 , (j = 1,2,...,n)\n",
    "\\end{aligned}\n",
    "\n",
    "$$L(x,\\alpha,\\beta) = f(x) + \\sum_{i=1}^m \\alpha_ih_i(x) + \\sum_{j=1}^n \\beta_jg_j(x) $$\n",
    "\n",
    "由之前的分析可知, 想要用拉格朗日乘子法, 求解不等式约束的优化问题, 需要满足 KKT 条件, 如下:\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\nabla_x L(x,\\alpha,\\beta) &= 0 &\\text{(1)}\\\\\n",
    "    \\beta_j g_j(x) &= 0, (j=1,2,...,n) &\\text{(2)}\\\\\n",
    "    h_i(x) &= 0 , (j=1,2,...,m) &\\text{(3)}\\\\\n",
    "    g_j(x) &\\leq 0 , (j=1,2,...,n) &\\text{(4)}\\\\\n",
    "    \\beta_j &\\geq 0 , (j=1,2,...,n) &\\text{(5)}\n",
    "\\end{aligned}\n",
    "\n",
    "满足 KKT 条件后极小化 Lagrangian 即可得到在不等式约束条件下的可行解. KKT 条件看起来很多, 其实很好理解:\n",
    "\n",
    "(1) ：拉格朗日取得可行解的必要条件；\n",
    "\n",
    "(2) ：这就是以上分析的一个比较有意思的约束，称作松弛互补条件；\n",
    "\n",
    "(3 ∼ 4) ：初始的约束条件；\n",
    "\n",
    "(5) ：不等式约束的 Lagrange Multiplier 需满足的条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拉格朗日对偶\n",
    "\n",
    "- [参考博客资料](http://www.cnblogs.com/ooon/p/5723725.html)\n",
    "- [参考博客资料](http://blog.pluskid.org/?p=682)\n",
    "\n",
    "在优化理论中, 根据目标函数和约束条件, 可以有很多分类. \n",
    "\n",
    "- 线性规划: 如果目标函数和约束条件都是 x 的线性函数\n",
    "- 二次规划: 如果目标函数是二次函数, 约束条件是线性函数\n",
    "- 非线性规划: 如果目标函数和约束条件都是 非线性函数\n",
    "\n",
    "每个线性规划都有一个对偶问题, 与之对应, 对偶问题之所以有用, 是因为它有很多良好的性质:\n",
    "\n",
    "- 对偶问题的对偶是原问题 \n",
    "- 无论原问题是否是凸的, 对偶问题都是凸优化问题\n",
    "- 对偶问题可以给出原问题的一个下界\n",
    "- 当满足一定条件时, 对偶问题完全等价于原问题\n",
    "\n",
    "对偶的基本概念是: 对于一个约束优化问题，找到其对偶问题，当弱对偶 (weak duality) 成立时，可以得到原始问题的一个下界。而如果强对偶 (strong duality) 成立，则可以直接求解对偶问题来解决原始问题。\n",
    "\n",
    "我们首先给出不等式优化的原始问题\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\min\\limits_x \\text{ } &: f(x) \\\\\n",
    "    s.t. &: h_i(x) = 0 , (i = 1,2,...,m)\\\\\n",
    "         &: g_j(x) \\leq 0 , (j = 1,2,...,n)\n",
    "         \\\\\n",
    "         \\\\\n",
    "    \\text{定义拉格朗日函数为: } L(x,\\alpha,\\beta) &= f(x) + \\sum_{i=1}^m \\alpha_ih_i(x) + \\sum_{j=1}^n \\beta_jg_j(x)\n",
    "\\end{aligned}\n",
    "\n",
    "根据以上拉格朗日函数可以得到一个重要结论\n",
    "\n",
    "$$ \\max\\limits_{\\alpha\\beta;\\beta_j\\geq0} L(x,\\alpha,\\beta) = f(x) $$\n",
    "\n",
    "上式很容易验证, 满足约束的解会使, $h_i(x)=0 (第二项消去)$, 并且有 $g_j(x)\\leq0$, 而当上式限定了 $\\beta_i\\geq0$ 后, 就可以得到 $\\sum_{j=1}^n \\beta_jg_j(x)$ 的最大值为0, 因此就得到上面的式子.\n",
    "\n",
    "然后我们的优化问题就变成如下原始问题, 并且把原始问题的解记为 $p^*$\n",
    "\n",
    "$$\\text{原始问题: } \\min\\limits_x f(x) = \\min\\limits_x \\max\\limits_{\\alpha\\beta;\\beta_j\\geq0} L(x,\\alpha,\\beta)$$\n",
    "\n",
    "为了方便找到**对偶问题**, 我们先定义一个对偶函数 (dual function), 然后把 $\\min \\text{和} \\max$ 调换就得到对偶问题.\n",
    "\n",
    "\\begin{aligned}\n",
    "D(\\alpha,\\beta) =&\\min\\limits_x L(x,\\alpha,\\beta) \\\\\n",
    "\\text{对偶问题: } &\\max\\limits_{\\alpha\\beta;\\beta_j\\geq0} \\min\\limits_x L(x,\\alpha,\\beta) \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "定义对偶问题的最优解 $d^*$ 即关于 $\\alpha, \\beta$的函数, \n",
    "\n",
    "$$d^* = \\max\\limits_{\\alpha\\beta;\\beta_j\\geq0} D(\\alpha,\\beta)$$\n",
    "\n",
    "> 对偶问题和原始问题的最优解并不相等, 而是满足的如下关系: $d^* \\leq p^*$\n",
    "\n",
    "> 直观地, 可以理解为最小的里最大的那个要比最大的中最小的那个要大.\n",
    "\n",
    "### 弱对偶性和强对偶性\n",
    "\n",
    "如同最开始所说的, 对偶问题就带来了原始问题的下限. $d^* \\leq p^*$这个性质叫做 **弱对偶性 (weak duality)**, 对于所有优化问题都成立, 即使原始问题非凸. 这里还有两个概念, **对偶间隔 (duality gap)**: $f(x) - D(\\alpha,\\beta)$; **最优对偶间隔 (optimal dualty gap)**: $p^* - d^*$\n",
    "\n",
    "与弱对偶性相对应的有一个**强对偶性(strong duality)**, 强对偶即满足 $d^* = p^*$. 强对偶是一个非常好的性质, 因为在强对偶成立的情况下, 可以通过求解对偶问题来得到原始问题的解.\n",
    "\n",
    "\n",
    "### 对偶问题总结\n",
    "\n",
    "> To summarize, for *any* optimization problem with differentiable objective and constaint functions for which strong duality obtains, any pair of primal and dual optimal points must satisfy the KKT conditions.\n",
    "\n",
    "> 任何满足强对偶性的优化问题, 只要其目标函数与约束函数可微, 任一对原始问题与对偶问题的解都是满足 KKT 条件的. 即满足强对偶性的优化问题中, 若 $x^∗$ 为原始问题的最优解，$\\alpha^∗, \\beta^∗$ 为对偶问题的最优解, 则可得 $x^∗,\\alpha^∗,\\beta^∗$ 满足 KKT 条件.\n",
    "\n",
    "> Summary, for any *convex* optimization problem with defferentiable objective and constraint functions, any points that satisfy the KKT conditions are primal and dual optimal, and have zero duality gap.\n",
    "\n",
    "> 对于任何凸优化问题, 如果目标函数和约束函数可微, 那么所有满足 KKT 条件的解, 都是原始问题和对偶问题的最优解, 并且其对偶间隔为0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回到 SVM 算法的推导\n",
    "\n",
    "在上面一节解释了拉格朗日函数, 对偶问题和 KKT 条件之后, 我们回到 SVM 的推导上来. 开始说到为了求最大间隔, 我们需要求解的问题如下:\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\min_{w,b} \\text{ } &: \\frac{1}{2}\\|w\\|^2 \\\\\n",
    "    s.t. &: y^\n",
    "    {(i)} (w^Tx^{(i)}+b) \\geq 1, (i=1,2,...,m)\n",
    "\\end{aligned}\n",
    "\n",
    "由此, 我们可以得到原始问题为\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\text{原始问题为 } \\min_{w,b} \\text{ } &: \\frac{1}{2}\\|w\\|^2 \\\\\n",
    "    s.t. &: - ( y^{(i)} (w^Tx^{(i)}+b) -1 ) \\leq 0, (i=1,2,...,m) \\\\\n",
    "    \\text{拉格朗日函数为 }&: L(w,b,a) = \\frac{1}{2} {\\|w\\|}^2 - \\sum_{i=1}^N a_i(y_i(w \\cdot x_i+b) - 1) \\\\\n",
    "    \\text{将约束条件带入, 可得 }&: \\max\\limits_{a_i\\geq0} L(w,b,a) = \\frac{1}{2}\\|w\\|^2 \\\\\n",
    "    \\text{所以原始问题就是 }&: p^* = \\min\\limits_{w,b} \\max\\limits_{a_i\\geq0} L(w,b,a) \\\\\n",
    "    \\text{相应的对偶问题 }&: d^* = \\max\\limits_{a_i\\geq0} \\min\\limits_{w,b} L(w,b,a) \n",
    "\\end{aligned}\n",
    "\n",
    "因为 SVM 的原始问题满足 slater 条件, 有强对偶性, 所以\n",
    "\n",
    "$$p^* = d^* = L(w^*,b^*,a^*)$$\n",
    "\n",
    "由于满足 Salter 条件（即满足强对偶），可知每对满足原始问题与对偶问题的解都满足 KKT 条件，即：\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\nabla_w L(w^*,b^*,a^*) &= w^* - \\sum_{i=1}^ma_i^*y_ix_i = 0 &\\text{(1)} \\\\\n",
    "    \\nabla_b L(w^*,b^*,a^*) &= - \\sum_{i=1}^ma_i^*y_i = 0 &\\text{(2)} \\\\\n",
    "    a_i^*(y_i^*(w^{*T}x_i+b^*)-1) &= 0 &\\text{(3)} \\\\\n",
    "    y_i(w^{*T}x_i+b^*) -1 &\\geq 0 &\\text{(4)} \\\\\n",
    "    a_i^* &\\geq 0, (i=1,2,...,m) &\\text{(5)}\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上面的 $(1)$ 可得\n",
    "\n",
    "$$w^* = \\sum_{i=1}^ma_i^*y_ix_i \\text{ (6)}$$\n",
    "\n",
    "由 $(3),(4),(5)$ 可以得到, 至少存在一个 $a_j>0$ 使 $y_j(w^{*T}x_j+b^*)-1=0$, 这个点 $x_j$ 就是所谓的支持向量;\n",
    "\n",
    "\\begin{aligned}\n",
    "                    b^* &= \\frac{1}{y_j} - w^{*T}x_j \\\\\n",
    "   \\text{代入(6)式 } b^* &= y_j - (\\sum_{i=1}^ma_i^*y_ix_i)^Tx_j \\\\\n",
    "                    b^* &= y_j - \\sum_{i=1}^ma_i^*y_i(x_i)^Tx_j \\\\\n",
    "                    b^* &= y_j - \\sum_{i=1}^ma_i^*y_i(x_i\\cdot x_j) \\text{ (7)}\n",
    "\\end{aligned}\n",
    "\n",
    "而对其它的点使得 $y_i(w^{*T}x_i+b^*) -1 > 0$ (即非支持向量), 则有 $a_i=0$, 因此这些点不会影响最终优化问题的结果, 也就是说支持向量就决定了 SVM 最终的超平面.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们考虑对偶问题的时候, 先处理 $\\min$ 的部分, 上面的 $(2),(6)$ 等于就是满足最小化的约束条件, 带入到拉格朗日函数中去\n",
    "\n",
    "$$\\max\\limits_{a_i\\geq0} \\min\\limits_{w,b} L(w,b,a)$$\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\min\\limits_{w,b} L(w,b,a) &= \\frac{1}{2}\\|w\\|^2 - \\sum_{i=1}^m a_i(y_i(w^Tx_i+b)-1) \\\\ \n",
    "                               &= \\frac{1}{2}w^Tw - \\sum_{i=1}^m a_iy_iw^Tx_i + \\sum_{i=1}^m a_iy_ib + \\sum_{i=1}^m a_i \\\\\n",
    "                               &= \\frac{1}{2}w^T \\sum_{i=1}^ma_iy_ix_i - \\sum_{i=1}^m a_iy_iw^Tx_i + \\sum_{i=1}^m a_iy_ib + \\sum_{i=1}^m a_i \\\\\n",
    "                               &= -\\frac{1}{2}w^T \\sum_{i=1}^ma_iy_ix_i + \\sum_{i=1}^m a_iy_ib + \\sum_{i=1}^m a_i \\\\\n",
    "                               &= - \\frac{1}{2} (\\sum_{i=1}^ma_iy_ix_i)^T \\sum_{i=1}^ma_iy_ix_i + \\sum_{i=1}^m a_iy_ib + \\sum_{i=1}^m a_i \\\\\n",
    "                               &= - \\frac{1}{2} \\sum_{i=1}^ma_iy_i(x_i)^T \\sum_{i=1}^ma_iy_ix_i + \\sum_{i=1}^m a_iy_ib + \\sum_{i=1}^m a_i \\\\\n",
    "                               &= - \\frac{1}{2} \\sum_{i=1}\\sum_{j=1}^m a_ia_jy_iy_j(x_i\\cdot x_j) + b\\sum_{i=1}^m a_iy_i + \\sum_{i=1}^m a_i \\\\\n",
    "    \\min\\limits_{w,b} L(w,b,a) &= - \\frac{1}{2} \\sum_{i=1}\\sum_{j=1}^m a_ia_jy_iy_j(x_i\\cdot x_j) + \\sum_{i=1}^m a_i \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "现在对偶问题就推导为如下, 求关于 $a$ 的极大值的问题:\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\max_{a} \\text{ } &: - \\frac{1}{2} \\sum_{i=1}\\sum_{j=1}^m a_ia_jy_iy_j(x_i\\cdot x_j) + \\sum_{i=1}^m a_i \\\\\n",
    "    s.t. &: a_i \\geq 0, (i=1,2,...,m) \\\\\n",
    "         &: \\sum_{i=1}^m a_iy_i = 0\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结一下最终的线性可分支持向量机学习算法\n",
    "\n",
    "\\begin{aligned}\n",
    "    &\\text{输入}:{线性可分数据集}  \\{(x_i,y_i)\\}_{i=1}^m \\\\\n",
    "    &\\text{(1)  构造约束最优化问题: } \\\\\n",
    "        &\\max_{a} \\text{ } : - \\frac{1}{2} \\sum_{i=1}\\sum_{j=1}^m a_ia_jy_iy_j(x_i\\cdot x_j) + \\sum_{i=1}^m a_i \\\\\n",
    "        & s.t.\\text{: }  a_i \\geq 0, (i=1,2,...,m) \\\\\n",
    "        & s.t.\\text{: }  \\sum_{i=1}^m a_iy_i = 0 \\\\\n",
    "    &\\text{(2) 求解得到} a^∗=(a^∗_1,a^∗_2,…,a^∗_m)^T , 求解一般采用SMO算法; \\\\\n",
    "    &\\text{(3) 根据之前的 KKT 条件, 求得} w^∗,b^∗ {首先选择} a_j^*>0 \\text{的支持向量}(x_j, y_j) \\\\\n",
    "        &\\text{} w^* = \\sum_{i=1}^ma_i^*y_ix_i \\\\\n",
    "        &\\text{ } b^* = y_j - \\sum_{i=1}^ma_i^*y_i(x_i\\cdot x_j) \\\\\n",
    "    &\\text{(4) 求得超平面} w^∗x+b^∗=0, \\text{对于新的观测数据} x, \\text{根据} f(x)=sign(w^∗⋅x+b^∗), \\text{判断其类别} y\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 核方法\n",
    "\n",
    "对偶问题由于性质良好一般比原始问题更容易求解, 在 SVM 中通过引入对偶问题可以将问题表示成数据的内积形式从而使得 kernel trick 的应用更加自然. 这是引入 Kernel 的一种思路.\n",
    "\n",
    "而另一种思路是, 因为上述所讲的一般 SVM 只能解决用线性超平面解决线性可分的问题, 当我们遇到线性不可分的数据时, 就需要将数据从低维空间映射到高维空间, 使其在高维空间中线性可分."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table><tr>\n",
    "<td><img src=\"images/svm_nonlinear_data.png\" width=\"300\"></td>\n",
    "<td><img src=\"images/svm_nonlinear_rotate.gif\" width=\"300\"></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一张图的数据可以表示为如下方程, 在 $(x_1,x_2)$ 这个二维空间中不是线性可分的\n",
    "\n",
    "$$a_!x_1+a_2x_1^2+a_3x_2+a_4x_2^2+a_5x_1x_2+a_6 = 0$$\n",
    "\n",
    "而当我们使得 $z_!=x_1, z_2=x_1^2, z_3=x_2, z_4=x_2^2, z_5=x_1x_2$, 将数据从二维映射到五维 ($\\phi : \\Re^2 \\rightarrow \\Re^5$) 后, 得可以得到一个超平面将数据成功的划分开了, 这就是用 Kernel方法 处理非线性问题的基本思路:\n",
    "\n",
    "$$\\sum_{i=1}^5 a_iz_i + a_6 = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上节的推导中, 我们得到最终的分类函数为:\n",
    "\n",
    "$$f(x) = \\sum_{i=1}^na_iy_i \\langle x_i,x \\rangle +b $$\n",
    "\n",
    "现在进过映射后的分类函数为:\n",
    "\n",
    "$$f(x) = \\sum_{i=1}^na_iy_i \\langle \\phi(x_i), \\phi(x) \\rangle +b $$\n",
    "\n",
    "相应的, 也是通过对偶问题解出 $a$. 但是这里有一个问题就是, 映射到高维之后, 维度的爆发式增长. 例如, 上面的例子, 原问题是二维的特征空间, 映射到可分的空间后, 就变成的五维. 映射后的控件维度可能变得很大, 甚至于无限大(高斯核), 这就使得 $\\phi(\\cdot)$ 的计算变得十分困难了.\n",
    "\n",
    "这里我们就可以引出 **核函数**, 通过核函数, 我们可以把高纬度十分困难的内积运算, 通过低纬度的内积运算来等同的表示.\n",
    "\n",
    "$$\\text{核函数 } k(x_1, x_2) = \\langle \\phi(x_1),\\phi(x_2) \\rangle$$\n",
    "$$f(x) = \\sum_{i=1}^na_iy_i k(x_!, x_2) +b$$\n",
    "\n",
    "> **核函数能够简化映射空间中的内积运算, 而 SVM 中的向量运算 \"恰好\" 是以内积的形式出现的**\n",
    "\n",
    "这样我们只需要选择合适的核函数, 就可以通过核方法(不用真的进行高维空间的内积运算), 简单的计算出分类函数.\n",
    "\n",
    "有一个简单例子的推导可以参见资料 http://blog.pluskid.org/?p=685\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下列举通常用到的核函数:\n",
    "\n",
    "- **多项式核** : $k(x_!,x_2)= (\\langle x_1,x_2 \\rangle + R)^d$\n",
    "\n",
    "- **高斯核** : $k(x_!,x_2)= \\exp(- \\frac{{\\|x_1 - x_2\\|}^2}{2\\sigma^2})$\n",
    "\n",
    "    高斯核会将原始空间映射到无穷多维, 但是当$\\sigma$比较大的时候, 高次特征上的权重会衰减的十分厉害, 实际上就相当于一个相对比较低维的空间, 当$\\sigma$ 取值比较小的时候, 超高维的空间可以有能力将任意数据线性划分, 但是会带来严重的过拟合问题. 总体而言, 通过对$\\sigma$的调参, 高斯核具有很高的灵活性, 是最常用的核函数.\n",
    "\n",
    "- **线性核** : $k(x_!,x_2)= \\langle x_1,x_2 \\rangle$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **除了 SVM 之外，任何将计算表示为数据点的内积的方法，都可以使用核方法进行非线性扩展。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 松弛变量\n",
    "\n",
    "松弛变量的引入是为了解决数据本来是线性可分的, 但是存在噪音. 对于这种偏离正常位置很远的数据点，我们称之为 outlier. 如下图黑圈的蓝点就是一个 outlier, 它偏离了自己原本所应该在的那个半空间. 下图显示是否忽略这个点的两种情况, 可以看到如果不忽略 outlier, 我们得到的分隔 margin 会更小. 甚至再极端情况下, outlier 偏离的更远了, 会导致数据线性不可分."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/svm_outlier2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了兼容 outlier 的影响, 我们需要对基础算法的约束条件做一点修改, 允许有的点可以跨过 decision bound.\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\text{原来的约束条件: }& y^{(i)} (w^T x^{(i)} + b) \\geq  1 , (i=1,2,...,n)\\\\\n",
    "    \\text{调整后的约束条件: }& y^{(i)} (w^T x^{(i)} + b) \\geq 1 - \\xi^{((i)}, (i=1,2,...,n)\n",
    "\\end{aligned}\n",
    "\n",
    "其中 $\\xi^(i) >=0$, 就被称为松弛变量 (slack variable). 当然如果允许任意大小的松弛变量, 那么任何分隔平面都可以满足原来的目标函数, 因此, 对目标函数也要进行修改, 考虑最小的松弛变量.\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\text{原来的目标函数: }& \\min \\frac{1}{2} \\|w\\|^2\\\\\n",
    "    \\text{调整后的目标函数: }&  \\min \\frac{1}{2} \\|w\\|^2 + C\\sum_{i=1}^n \\xi^{(i)}\n",
    "\\end{aligned}\n",
    "\n",
    "其中 $C$ 是一个参数，用于控制目标函数中两项（\"寻找 margin 最大的超平面\"和\"保证数据点偏差量最小\"）之间的权重.\n",
    "\n",
    "所以我们得到加入了松弛变量之后的优化问题为:\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\min_{w,b} \\text{ } &: \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^n \\xi^{(i)}\\\\\n",
    "    s.t. &: - (y^\n",
    "    {(i)} (w^Tx^{(i)}+b) -1 + \\xi^{(i)}) \\leq 0, (i=1,2,...,n) \\\\\n",
    "    &: - \\xi^{(i)} \\leq 0, (i=1,2,...,n)\n",
    "\\end{aligned}\n",
    "\n",
    "通过拉格朗日乘子法得到拉格朗日函数为:\n",
    "\n",
    "$$L(w, b, a, r, \\xi) = \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^n \\xi_i - \\sum_{i=1}^n a_i(y_i (w^Tx_i+b) -1 + \\xi_i) - \\sum_{i=1}^n r_i\\xi_i$$\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\nabla_w L(w^*,b^*,a^*, r^*) &= w^* - \\sum_{i=1}^ma_i^*y_ix_i = 0 &\\text{(1)} \\\\\n",
    "    \\nabla_b L(w^*,b^*,a^*, r^*) &= - \\sum_{i=1}^ma_i^*y_i = 0 &\\text{(2)} \\\\\n",
    "    \\nabla_{\\xi_i} L(w^*,b^*,a^*, r^*) &= C_i - a_i - r_i = 0 &\\text{(3)}\n",
    "\\end{aligned}\n",
    "\n",
    "再将 $(1)$ 求得的 $w$ 和 $(2)$ 代入 $L$ 中并化简就可以得到和原来一样的目标函数, 不过由于 $(3)$, 我们得到 $ a_i = C - r_i$, 而因为拉格朗日乘子 $r_i \\geq 0$, 所以得到约束条件 $0 \\leq a_i \\leq C$\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\max_{a} \\text{ } &: - \\frac{1}{2} \\sum_{i=1}\\sum_{j=1}^m a_ia_jy_iy_j(x_i\\cdot x_j) + \\sum_{i=1}^m a_i \\\\\n",
    "    s.t. &: 0 \\leq a_i \\leq C, (i=1,2,...,m) \\\\\n",
    "         &: \\sum_{i=1}^m a_iy_i = 0\n",
    "\\end{aligned}\n",
    "\n",
    "和之前的问题对比一下, 唯一的区别就是 $a_i$ 增加了一个 $C$ 的上限. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SMO\n",
    "\n",
    "SMO (Sequential Minimal Optimization) 是一种求解 SVM 问题的优化算法.\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\min_{a} \\text{ } &:  \\frac{1}{2} \\sum_{i=1}\\sum_{j=1}^n a_ia_jy_iy_j(x_i\\cdot x_j) - \\sum_{i=1}^n a_i \\\\\n",
    "    s.t. &: 0 \\leq a_i \\leq C, (i=1,2,...,n) \\\\\n",
    "         &: \\sum_{i=1}^n a_iy_i = 0\n",
    "\\end{aligned}\n",
    "\n",
    "为了从以上问题, 求解最优的 $(a_1,a_2,...,a_n)$, 由于目标函数为凸函数, 一般的优化方法都是通梯度方法一次优化一个变量. 但是考虑上述的约束条件, 假设选择优化变量 $a_i$, 当固定其它 n-1 个变量时, 因为约束条件 $\\sum_{i=1}^n y_ia_i=0$, $a_1=-y_1\\sum_{i=2}^ny_ia_i$, 所以导致 $a_1$也固定下来了, 所以对于特定的 SVM 问题的优化, 我们就选择一次优化两个变量.\n",
    "\n",
    "假设我们现在选择优化变量 $a_1,a_2$, 由于剩下的 n-2 个变量固定了,就可以简化目标函数为只关于 $a_1,a_2$ 的二元函数.\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\Psi(a_1,a_2) &= \\frac{1}{2} \\sum_{i=1}\\sum_{j=1}^n a_ia_jy_iy_jk(x_i,x_j) - \\sum_{i=1}^n a_i \\\\\n",
    "                  &=\\frac{1}{2}(y_1^2a_1^2k_{11}+y_2^2a_2^2k_{22}+2y_1y_2a_1a_2k_{12}) - (a_1+a_2) + \\frac{1}{2}(2y_1a_1\\sum_{i=3}^ny_ia_ik_{i1}+2y_2a_2\\sum_{i=3}^ny_ia_ik_{i2}) + CONSTANT\\\\\n",
    "                  &=\\frac{1}{2}y_1^2a_1^2k_{11}+\\frac{1}{2}y_2^2a_2^2k_{22}+y_1y_2a_1a_2k_{12}-a_1-a_2+y_1a_1\\sum_{i=3}^ny_ia_ik_{i1}+y_2a_2\\sum_{i=3}^ny_ia_ik_{i2} +CONSTANT \\\\\n",
    "                  &=\\frac{1}{2}y_1^2a_1^2k_{11}+\\frac{1}{2}y_2^2a_2^2k_{22}+y_1y_2a_1a_2k_{12}-a_1-a_2+y_1a_1V_1+y_2a_2V_2 +CONSTANT &(1)\n",
    "\\end{aligned}\n",
    "\n",
    "由等式约束, $\\sum_{i=1}^ny_ia_i=0 $\n",
    "\n",
    "\\begin{aligned}\n",
    "    a_1y_1+a_2y_2&=-\\sum_{i=3}^ny_ia_i = \\xi \\\\\n",
    "    a_1 &= y_1(\\xi - a_2y_2) &(2)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把 $(2)$ 式带入 $(1)$ 式, 并且由 $y_1^2=y_2^2=1$ 化简得关于$a_2$的一元二次函数 (求极值的目标函数, 可以忽略常数项):\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\min &\\Psi(a_1,a_2) \\\\\n",
    "    \\Psi(a_2) &=\\frac{1}{2}(\\xi-a_2y_2)^2k_{11}+\\frac{1}{2}a_2^2k_{22}+y_2(\\xi-a_2y_2)a_2k_{12}-(\\xi-a_2y_2)y_1-a_2+(\\xi-a_2y_2)V_1+y_2a_2V_2 \\\\\n",
    "              &=\\frac{1}{2}(\\xi^2+a_2^2y_2^2-2\\xi a_2y_2)k_{11}+\\frac{1}{2}a_2^2k_{22}+y_2(\\xi-a_2y_2)a_2k_{12}-(\\xi-a_2y_2)y_1-a_2+(\\xi-a_2y_2)V_1+y_2a_2V_2\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求极值\n",
    "\n",
    "$$\\nabla_{a_2}\\Psi(a_2) = k_{11}y_2^2a_2 - \\xi k_{11}y_2 + k_{22}a_2+\\xi k_{12}y_2 - 2k_{12}y_2^2a_2 +y_1y_2 - 1 - y_2V_1 + y_2V_2 = 0$$\n",
    "\n",
    "$$ (k_{11} + k_{22} - 2k_{12})a_2 = y_2[\\xi (k_{11}-k_{12}) +y_2-y_1+V_1-V_2] \\text{ }\\text{ }\\text{ (3)} $$\n",
    "\n",
    "接下来带入化简$(3)$式:\n",
    "\n",
    "$$a_1^{new}y_1 + a_2^{new}y_2 = a_1^*y_1+a_2^*y_2 = \\xi \\text{ }\\text{, }a^*\\text{是原始值}$$\n",
    "$$V_1 = \\sum_{i=3}^ny_ia_ik_{1i} = f(x_1) - b - y_1a_1k_{11} - y_2a_2k_{12}$$\n",
    "$$V_2 = \\sum_{i=3}^ny_ia_ik_{2i} = f(x_2) - b - y_1a_1k_{21} - y_2a_2k_{22}$$\n",
    "\n",
    "\\begin{aligned}\n",
    "    (k_{11} + k_{22} - 2k_{12})a_2^{new} &= y_2[\\xi (k_{11}-k_{12}) +y_2-y_1+V_1-V_2] \\\\\n",
    "    & = y_2[(a_1^*y_1+a_2^*y_2)(k_{11}-k_{12}) +y_2-y_1+V_1-V_2] \\\\\n",
    "    & = y_2[a_1^*k_{11}y_1-a_1^*k_{12}y_1+a_2^*k_{11}y_2-a_2^*k_{12}y_2+y_2-y_1+f(x_1)-b-y_1a_1^*k_{11}-y_2a_2^*k_{12}-(f(x_2)-b-y_1a_1^*k_{21}-y_2a_2^*k_{22})] \\\\\n",
    "    & = y_2[a_2^*k_{11}y_2+a_2^*k_{22}y_2-2a_2^*k_{12}y_2]+y_2[(f(x_1)-y_1)-(f(x_2)-y_2)] \\\\\n",
    "    & = (k_{11}+k_{22}-2k_{12})a_2^*+y_2[(f(x_1)-y_1)-(f(x_2)-y_2)] \\\\\n",
    "    a_2^{new,unclipped} &=a_2^* + \\frac{y_2[(f(x_1)-y_1)-(f(x_2)-y_2)]}{k_{11} + k_{22} - 2k_{12}} \n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们做以下定义\n",
    "\n",
    "$$\\text{预测值与真实值之差为: }E_i = f(x_i) - y_i$$\n",
    "$$\\eta = k_{11} + k_{22} - 2k_{12}$$\n",
    "$$a_2^{new,unclipped} =a_2^{old} + y_2\\frac{E_1-E_2}{\\eta} \\text{ }\\text{ }\\text{ (4)}$$\n",
    "\n",
    "再通过约束条件$\\sum_{i=1}^na_iy_i = 0$, 就可以求得 $a_1^{new, unclipped}$, 这里我们就可以求出没有考虑约束条件 $(0\\leq a_i\\leq C)$ 的原始解, 接下来,我们需要对原始解进行范围上的修剪."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/svm_smo.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$0 \\leq a_i \\leq C, (i=1,2,...,n)$$\n",
    "\\begin{cases}\n",
    "   a_1 - a_2 = \\xi &\\text{if } y_1 = y_2  \\\\\n",
    "   a_1 + a_2 = \\xi &\\text{if } y_1 \\neq y_2\n",
    "\\end{cases}\n",
    "\n",
    "所以最优解必然在区间[0,C]x[0,C] 的对角线上, 下面分两种情况, 来考虑 $a_2$的取值范围:\n",
    "\n",
    "$$L \\leq a_2 \\leq H$$\n",
    " \n",
    "当 $y_1 \\neq y_2$ 时, $a_2 = a_1 - \\xi$\n",
    "\n",
    "\\begin{aligned}\n",
    "    0 \\leq a_1 \\leq C \\\\\n",
    "    -\\xi \\leq a_1 - \\xi \\leq C-\\xi \\\\\n",
    "    -\\xi \\leq a_2 \\leq C-\\xi \\\\\n",
    "    0 \\leq a_2 \\leq C \n",
    "\\end{aligned}\n",
    "\n",
    "$$\\max(0,-\\xi)\\leq a_2 \\leq \\min(C,C-\\xi)$$\n",
    "\n",
    "所以当 $y_1 \\neq y_2$ 时, \n",
    "\n",
    "\\begin{cases}\n",
    "   L = \\max(0, a_2^{old} - a_1^{old}) \\\\\n",
    "   H = \\min(C, C + a_2^{old} - a_1^{old})\n",
    "\\end{cases}\n",
    "\n",
    "同样的, 当 $y_1 = y_2$ 时, \n",
    "\n",
    "\\begin{cases}\n",
    "   L = \\max(0, a_2^{old} + a_1^{old} - C) \\\\\n",
    "   H = \\min(C, a_2^{old} + a_1^{old})\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综合之前的原始解 和 取值范围, 我们就可以得出最终的 $a_2^{new}$\n",
    "\n",
    "$$a_2^{new}= \n",
    "\\begin{cases}\n",
    "   H &\\text{if } a_2^{new,unclipped} > H  \\\\\n",
    "   a_2^{new,unclipped} &\\text{if } L \\leq a_2^{new,unclipped} \\leq H  \\\\\n",
    "   L &\\text{if } a_2^{new,unclipped} < L\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "进一步可求得另一个参数的新值\n",
    "\n",
    "$$a_1^{new} = a_1^{old}+y_1y_2(a_2^{old}-a_2^{new})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMO算法中变量的选择\n",
    "\n",
    "SMO 每次迭代求解两个分量的过程，是通过启发式的方法选择合适的分量来进行优化. 外循环选择违反 KKT 条件的样本的拉格朗日乘子 $a_i$, 作为第一个需要优化的分量, 内循环选择的第二个分量由 $\\max|E_1-E_2|$来决定, 这样带来第二个分量更大的变化, 加速优化."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 支持向量机回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/svm_regression.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
