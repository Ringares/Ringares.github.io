---
layout: post
title: "决策树的生成"
description: "Decision Tree"
category: Develop
tags: [git]
---

## 决策树的生成

一般来说, 一颗决策树包含一个根结点, 若干中间结点, 和若干叶结点. 

- 叶结点 对应决策结果
- 其它结点 对应一个属性测试

每个结点包含的样本集合根据属性测试的结果被划分到子结点中; 根结点包含所有样本. 

决策树学习的目的是生成一个泛化能力强的决策树, 其基本流程遵循"分而治之"策略.

### 生成算法

- 输入: 训练集 $D=\{(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)\}$ 和 属性集 $A=\{a_1, a_2, ..., a_d\}$
- TreeGenerate(D, A)
- 输出: 以 node 为根结点的决策树

```
    1. 生成结点 node
    2. if 训练集D中样本全都属于同一个类别C then
           将node标记为属于C类别的叶结点 return (1)
       endif
    3. if 属性集A为空集 OR D所有样本在所有属性A上的取值都相同 then
           将node标记叶结点, 其类别为D中样本数最多的那个分类C return (2)
       endif
    4. 从属性集A中选择最优划分属性 a*
    5. for 属性a*中的每一个值 a*_v do
           在 node 下生成一个分支结点, 令D_v表示D的子集, 子集中的样本的属性 a* 的取值为 a*_v
           if D_v是空集 then
               将分支结点标记为叶结点, 类别C= D中样本最多的分类 return (3)
           else
               TreeGenerate(D_v, A\{a*}) // A中除去 a* 以外的集合 
           end if

       endfor
```

如上所述, 在决策树的基本算法中, 有三种情况会导致递归返回.
- (1) 当前结点样本都属于同一类别, 无需划分
- (2) 当前属性集为空或所有样本在所有属性上取值相同, 无法划分
- (3) 当前样本集为空, 无法划分

## 划分选择

> 决策树算法的关键在于如何选择最优的划分属性

> 一般而言, 随着划分的过程不断进行, 我们希望分支结点所包含的样本尽可能的属于同一类别

> 也就是结点的纯度越来越高

### Information Entropy 信息熵

信息熵用来度量集合样本纯度, 假设当前集合 $D$ 中, 第 $k$ 类样本的比例为 $p_k(k=1,2,...,|y|)$, 则 $D$ 的信息熵定义为

$$Ent(D) = - \sum_{k=1}^{|y|}p_klog_2p_k$$

$Ent(D)$ 的值越小, 集合 $D$ 纯度越高

### Information Gain 信息增益 [ID3]

假设离散属性 $a$ 有 $V$ 个可能的取值 $\{a^1, a^2,..., a^V\}$, 如果用属性 $a$ 来对样本进行划分, 就会产生 $V$ 个分支结点, 其中第 $v$ 个分支结点包含了 $D$ 中所有在属性 $a$ 上取值为 $a^v$ 的样本, 这个子集记为 $D^v$

那么我们可以算出每个分支结点下的信息熵, 并且**因为每个分支结点所含的样本数量不同**, 需要赋予权重 

$$|D^v|/|D|$$

由此可以计算出, 用属性 $a$ 进行划分所获得的信息增益.

$$Gain(D,a) = Ent(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)$$

信息增益越大, 说明用这个属性进行划分所带来的纯度提升越大

> ID3 决策树算法就是基于信息增益, 用信息增益最大的属性来进行划分.

> $a_* = \arg\max\limits_{a\in{A}}  Gain(D, a)$

> 一般来说信息增益偏好于选择取值数目多的属性, 因为可取值越多, 分支下的样本就越纯, 进而熵越小, 信息增益越大.


### Gain Ratio 增益率 [C4.5]

为了减少信息增益对多取值属性的偏好所带来的不利影响, 著名的 **C4.5 决策树算法** 使用增益率来选取最优划分属性.

$$Gain\_ratio = \frac{Gain(D,a)}{IV(a)}$$
$$IV(a) = - \sum_{v=1}^V \frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}$$

$IV(a)$ 被称为属性 $a$ 的固有值 (intrinsic value), 可取值数目越多 (V越大), $IV(a)$ 的值通常会越大 

> 增益率策略通常对可取值数目较少的属性有所偏好; 所以 **C4.5** 是先选取信息增益大于平均水平的属性, 再在其中选择增益率最高的

### Gini Index 基尼指数 [CART]

CART 决策树 (Classification And Regression Tree) 使用基尼指数来选择划分属性, 数据集 $D$ 的基尼值表示为

$$Gini(D) = \sum_{k=1}^{|y|}\sum_{k'\not = k}^{|y|}p_k^2$$
$$Gini(D) = 1 - \sum_{k=1}^{|y|}p_k^2$$

直观来说 $Gini(D)$ 反应了从数据集中随机抽取两个样本, 其类别不一致的概率. 所以 $Gini(D)$ 越小, 数据集纯度越高. 我们可以把用属性 $a$ 的基尼指数定义为

$$Gini\_index(D,a) = \sum_{v=1}^V \frac{|D^v|}{|D|}Gini(D^v)$$

> 因此, 我们选择划分后基尼指数最大的属性作为最优划分属性

> $a_* = \arg\min\limits_{a \in A} Gini\_index(D,a) $

## 剪枝处理

剪枝是决策树算法中对抗"过拟合"的主要手段, 通过主动去掉一些分支来降低过拟合的风险. 通过判断剪枝前后, 树的泛化性有没有提升, 来决定是否在这个结点进行剪枝, 直接标记为叶结点. 而我们可以采用预留法, 将数据集划分为 训练集 (training set)和 验证集(cv set), 通过模型对验证集的拟合程度, 来判断树的泛化性能.

### Pre-pruning 预剪枝

预剪枝是在树的生成过程中, 每次用属性划分分支结点后, 用验证集来判断划分前后的泛化性能, 如果验证集的精度没有提升, 就不进行划分, 直接标记为叶结点. (如果划分前后, 验证集的精度没有变化, 根据奥卡姆剃刀, 简单的模型更好; 如果考虑保守策略, 也可以不剪枝, 继续划分下去)

因为在树生成过程中, 很多分支有可能就没有展开, 所以预剪枝的时间开销比较小, 但是通常模型比较简单, 所以有过拟合的风险.

### Post-pruning 后剪枝

后剪枝是在树生成后, 逆向的一个个检查分支结点, 同样的, 判断分支结点转为叶结点前后, 验证集的精度是否有提高.

通常后剪枝会保留更多的分支, 后剪枝决策树的过拟合风险较小, 但是训练的时间开销比预剪枝和未剪枝的决策树都要大.


```python
## 连续值处理
```


```python
## 缺失值处理
```


```python
## 多属性决策树
```


```python
## 随机森林
```


